<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><!-- InstanceBegin template="/Templates/frame_content.dwt" codeOutsideHTMLIsLocked="false" -->
<head>
<!-- InstanceBeginEditable name="doctitle" -->
<title>Untitled Document</title>
<!-- InstanceEndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable --> 
<link href="../../css/fc.css" rel="stylesheet" type="text/css">
<style type="text/css">
<!--
.bg {
	background-image: url(../../images/images/main__11.gif);
	background-position: right bottom;
	background-repeat: no-repeat;
	background-attachment: fixed;
}
-->
</style>
</head>

<body >
<table width="100%" height="100%" border="0" cellpadding="4" cellspacing="0">
  <tr>
    <td valign="top"><!-- InstanceBeginEditable name="text" -->
      <table width="100%" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td class="FCcontent">　　例如图5.1表示对一幅道路图像按路面与非路面分类可用两种不同做法，其中左图是在图像中路面区与非路面中各找一个窗口，将其中每个象素分别作为这两类的训练样本集，用这两个样本集在特征空间的分布参数进行设计。而无监督学习方法则不同，它不预先选择样本类别的样本集，而是将整幅图的像素都作为待分类样本集，通过它们在特征空间中表现出来的聚类现象，把不同类别划分开，图5.1的有监督学习中，样本集分布呈现交迭情况，而无监督学习方法由于没有类别样本指导，无法确定它们的交迭情况，只能按分布的聚类情况进行划分。在类似于该例的实际应用问题中，预先选定不同类别的样本往往不可能，如时间不允许，或无法用人工干予等因素。另外在某些有监督学习方法中，也往往需要利用聚类方法将样本按其分布划分成若干子类等。聚类方法就是无监督学习方法的一个内容，它是经常应用的一门技术。</td>
        </tr>
        <tr>
          <td align="center" class="FCcontent"><img src="../../images/image_content/5/5_1002.gif" width="400" height="305"><br>
            图 5.1 </td>
        </tr>
        <tr>
          <td class="FCcontent">　　<span class="spe">人们日常生活中经常要观察事物与分析事物，从中寻找其规律性，这就是非监督学习方法要解决的问题。例如人们见到图5.1的道路图时，会发现中间有一条带与图中其它区域不同，见到图5.3会发现在这个二维空间中有数据显现出聚成两类的现象。这就是事物(对我们来说就是数据集)自身体现出的一些规律性，非监督学习方法就是寻找数据集中体现出来的规律性。从中我们可以强调非监督学习与有监督学习方法的以下几种不同点：<br>
            　　1． 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律；而非监督学习没有训练集这一说，只有一组数据，在该组数据集内寻找规律。<br>
            　　2． 有监督学习方法的目的就是识别事物，识别的结果表现在给待识别数据加上了标号。因此训练样本集必须由带标号的样本组成。而非监督学习方法只有要分析的数据集本身，预先没有什么标号。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不以与某种预先的分类标号对上号为目的。例如图5.1道路图像，有监督学习方法的目的是找到“道路”，而非监督学习方法则只是将中间一条带状区域区分开来，本质上讲与“道路”这个标号没有关系。<br>
            　　3． 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。这一点是比有监督学习方法的用途要广泛。譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于非监督学习方法的范畴。<br>
            　　4． 用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。应该说后者从方法上讲不是一种学习方法。因此用K-L变换找主分量不属于非监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。在人工神经元网络中寻找主分量的方法属于非监督学习方法。<br>
            　　以上四点是对非监督学习方法的定义，及与有监督学习方法的区别。</span><br>
            　　无监督学习方法可以分成两大类，一类为基于概率密度函数估计的直接方法，指设法找到各类别在特征空间的分布参数再进行分类。另一类称为基于样本间相似性度量的间接聚类方法，其原理是设法定出不同类别的核心或初始类核，然后依据样本与这些核心之间的相似性度量将样本聚集成不同类别。下面分别讨论这两种方法。<br>
            　　<span class="spe">最常用的基于概率密度估计的直接方法的例子是直方图方法。例如我们统计一所学校中学生身高分布就往往可采用直方图方法，把身高划分成一段段，如1米到1米75算一段，然后对每一段统计身高在此范围内的学生数，得到直方图。如果这个学校的男女学生数目相近，则我们就会发现该直方图会体现出有两个分布高峰。那么找到两高峰中的谷点，就会将学生划分成两类。<br>
            　　因此，使用概率统计方法的关键是能找出各个峰值区，这就是5.2节中的主要内容。另一种方法则在5.3节中再进一步讨论。</span><br></td>
        </tr>
      </table>
      <!-- InstanceEndEditable --></td>
  </tr>
</table>
</body>
<!-- InstanceEnd --></html>
